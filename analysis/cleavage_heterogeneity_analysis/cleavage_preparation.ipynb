{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleavage heterogeneity analysis preparation\n",
    "\n",
    "**Purpose:** To prepare data for an analysis of the factors mediating cleavage site heterogeneity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i notebook_setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from paper_utilities import helpers, motifs, cleavage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = \"/projects/b1080/eks/polyadenylation/yeast\"\n",
    "OUTDIR  = os.path.join(PROJECT, 'manuscript', 'analysis', 'resources')\n",
    "os.makedirs(OUTDIR, exist_ok = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process golden sites from *S. cerevisiae* unclustered data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload and process prediction data for golden sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11673, 18)\n"
     ]
    }
   ],
   "source": [
    "gold_data = pd.read_csv(os.path.join(OUTDIR, \"scer_golden_dist5.polya_comprehensive.txt\"), sep = \"\\t\")\n",
    "print(gold_data.shape)\n",
    "\n",
    "gold_data['label'] = gold_data.apply(lambda row : f'{row.name:06d}:{row.chrom}:{row.start}:{row.strand}', axis = 1)\n",
    "\n",
    "gold_data['long_sequence'] = gold_data['sequence']\n",
    "gold_data['sequence'] = gold_data['long_sequence'].apply(lambda x : x[int((len(x)-500)/2):int((len(x)+500)/2)])\n",
    "\n",
    "gold_data['ctr_cleavage']   = 250\n",
    "gold_data['readvec']        = gold_data['readvec'].apply(lambda x : np.asarray([float(_) for _ in x.strip(\"][\").split(\",\")]))\n",
    "gold_data['observed_norm']  = gold_data['readvec'].apply(lambda x: x / np.sum(x))\n",
    "gold_data['predicted_norm'] = gold_data['cleavage_norm'].apply(lambda x : np.asarray([float(_) for _ in x.strip(\"][\").split(\",\")]))\n",
    "\n",
    "gold_data['observed_entropy']  = gold_data['observed_norm'].apply(lambda x : cleavage.calculate_entropy_from_vector(x))\n",
    "gold_data['predicted_entropy'] = gold_data['predicted_norm'].apply(lambda x : cleavage.calculate_entropy_from_vector(x))\n",
    "\n",
    "gold_data['supporting_reads_log1p'] = np.log10(gold_data['supporting_reads'] + 1)\n",
    "gold_data['window_reads_log1p']     = np.log10(gold_data['window_reads'] + 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify the relative position of each site within a gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11673it [00:01, 9623.46it/s]\n"
     ]
    }
   ],
   "source": [
    "gold_sites = {}\n",
    "\n",
    "for i,row in tqdm.tqdm(gold_data.iterrows()):\n",
    "    \n",
    "    rowkey = (row['gene'],row['chrom'],row['strand'])\n",
    "    \n",
    "    if not (rowkey in gold_sites):\n",
    "        gold_sites[rowkey] = [row['start']]\n",
    "    else:\n",
    "        gold_sites[rowkey].append(row['start'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_sites_labels = {}\n",
    "\n",
    "for rk,pl in gold_sites.items():\n",
    "    \n",
    "    gold_sites_labels[rk] = {}\n",
    "    \n",
    "    if (len(pl) == 1):\n",
    "        gold_sites_labels[rk][pl[0]] = 'single'\n",
    "        \n",
    "    else:\n",
    "        spl = sorted(pl)\n",
    "        g,c,s = rk\n",
    "            \n",
    "        gold_sites_labels[rk][spl[0]] = 'first' if (s == '+') else 'last'\n",
    "        gold_sites_labels[rk][spl[-1]] = 'last' if (s == '+') else 'first'\n",
    "\n",
    "        for p in spl[1:-1]:\n",
    "            gold_sites_labels[rk][p] = 'middle'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "middle    4752\n",
      "last      2869\n",
      "first     2869\n",
      "single    1183\n",
      "Name: position, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "gold_data['position'] = gold_data.apply(lambda row : gold_sites_labels.get((row['gene'],row['chrom'],row['strand']), {}).get(row['start'], 'NA'), axis = 1)\n",
    "print(gold_data['position'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabulate motif occurrences and their location and frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTDIR, 'motif_definitions.scer.6mers.distance.pickle'), mode = 'rb') as handle:\n",
    "    hamming_definitions = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_motifs = {\n",
    "    \n",
    "    'UA_d0' : {\n",
    "        'motifs'   : hamming_definitions['priority']['TA/TA-rich_d0'],\n",
    "        'regions'  : [(-90,-30),(-90,-50),(-50,-30),(-105,-90),(-90,-50),(-50,-30),(-60,-45),(-45,-30),(-30,-15),(-15,0),(0,15),(15,30),(30,45),(-15,15),(-30,30)],\n",
    "        'dir_pref' : ['max','max','max','max','max','max','max','max','max','max','min','min','min','max','max'],\n",
    "    },\n",
    "    'UA_d1' : {\n",
    "        'motifs'   : hamming_definitions['priority']['TA/TA-rich_d0'] + hamming_definitions['priority']['TA/TA-rich_d1'],\n",
    "        'regions'  : [(-90,-30),(-90,-50),(-50,-30),(-105,-90),(-90,-50),(-50,-30),(-60,-45),(-45,-30),(-30,-15),(-15,0),(0,15),(15,30),(30,45),(-15,15),(-30,30)],\n",
    "        'dir_pref' : ['max','max','max','max','max','max','max','max','max','max','min','min','min','max','max'],\n",
    "    },\n",
    "    'UA_d2' : {\n",
    "        'motifs'   : hamming_definitions['priority']['TA/TA-rich_d0'] + hamming_definitions['priority']['TA/TA-rich_d1'] + hamming_definitions['priority']['TA/TA-rich_d2'],\n",
    "        'regions'  : [(-90,-30),(-90,-50),(-50,-30),(-105,-90),(-90,-50),(-50,-30),(-60,-45),(-45,-30),(-30,-15),(-15,0),(0,15),(15,30),(30,45),(-15,15),(-30,30)],\n",
    "        'dir_pref' : ['max','max','max','max','max','max','max','max','max','max','min','min','min','max','max'],\n",
    "    },\n",
    "    \n",
    "    'A_d0' : {\n",
    "        'motifs'   : hamming_definitions['priority']['A-rich_d0'],\n",
    "        'regions'  : [(-60,-15),(-45,-15),(-60,-45),(-45,-30),(-30,-15),(-15,0),(0,15),(15,30),(30,45),(-15,15),(-30,30)],\n",
    "        'dir_pref' : ['max','max','max','max','max','max','min','min','min','max','max'],\n",
    "    },\n",
    "    'A_d1' : {\n",
    "        'motifs'   : hamming_definitions['priority']['A-rich_d0'] + hamming_definitions['priority']['A-rich_d1'],\n",
    "        'regions'  : [(-60,-15),(-45,-15),(-60,-45),(-45,-30),(-30,-15),(-15,0),(0,15),(15,30),(30,45),(-15,15),(-30,30)],\n",
    "        'dir_pref' : ['max','max','max','max','max','max','min','min','min','max','max'],\n",
    "    },\n",
    "    'A_d2' : {\n",
    "        'motifs'   : hamming_definitions['priority']['A-rich_d0'] + hamming_definitions['priority']['A-rich_d1'] + hamming_definitions['priority']['A-rich_d2'],\n",
    "        'regions'  : [(-60,-15),(-45,-15),(-60,-45),(-45,-30),(-30,-15),(-15,0),(0,15),(15,30),(30,45),(-15,15),(-30,30)],\n",
    "        'dir_pref' : ['max','max','max','max','max','max','min','min','min','max','max'],\n",
    "    },\n",
    "    \n",
    "    'U_d0' : {\n",
    "        'motifs'   : hamming_definitions['priority']['T-rich_d0'],\n",
    "        'regions'  : [(-15,15),(-30,0),(0,30),(-60,-45),(-45,-30),(-30,-15),(-15,0),(0,15),(15,30),(30,45)],\n",
    "        'dir_pref' : ['max','max','min','max','max','max','max','min','min','min'],\n",
    "    },\n",
    "    'U_d1' : {\n",
    "        'motifs'   : hamming_definitions['priority']['T-rich_d0'] + hamming_definitions['priority']['T-rich_d1'],\n",
    "        'regions'  : [(-15,15),(-30,0),(0,30),(-60,-45),(-45,-30),(-30,-15),(-15,0),(0,15),(15,30),(30,45)],\n",
    "        'dir_pref' : ['max','max','min','max','max','max','max','min','min','min'],\n",
    "    },\n",
    "    'U_d2' : {\n",
    "        'motifs'   : hamming_definitions['priority']['T-rich_d0'] + hamming_definitions['priority']['T-rich_d1'] + hamming_definitions['priority']['T-rich_d2'],\n",
    "        'regions'  : [(-15,15),(-30,0),(0,30),(-60,-45),(-45,-30),(-30,-15),(-15,0),(0,15),(15,30),(30,45)],\n",
    "        'dir_pref' : ['max','max','min','max','max','max','max','min','min','min'],\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UA_d0 in (-90, -30) with direction preference `max` [N = 2]\n",
      "UA_d0 in (-90, -50) with direction preference `max` [N = 2]\n",
      "UA_d0 in (-50, -30) with direction preference `max` [N = 2]\n",
      "UA_d0 in (-105, -90) with direction preference `max` [N = 2]\n",
      "UA_d0 in (-90, -50) with direction preference `max` [N = 2]\n",
      "UA_d0 in (-50, -30) with direction preference `max` [N = 2]\n",
      "UA_d0 in (-60, -45) with direction preference `max` [N = 2]\n",
      "UA_d0 in (-45, -30) with direction preference `max` [N = 2]\n",
      "UA_d0 in (-30, -15) with direction preference `max` [N = 2]\n",
      "UA_d0 in (-15, 0) with direction preference `max` [N = 2]\n",
      "UA_d0 in (0, 15) with direction preference `min` [N = 2]\n",
      "UA_d0 in (15, 30) with direction preference `min` [N = 2]\n",
      "UA_d0 in (30, 45) with direction preference `min` [N = 2]\n",
      "UA_d0 in (-15, 15) with direction preference `max` [N = 2]\n",
      "UA_d0 in (-30, 30) with direction preference `max` [N = 2]\n",
      "UA_d1 in (-90, -30) with direction preference `max` [N = 38]\n",
      "UA_d1 in (-90, -50) with direction preference `max` [N = 38]\n",
      "UA_d1 in (-50, -30) with direction preference `max` [N = 38]\n",
      "UA_d1 in (-105, -90) with direction preference `max` [N = 38]\n",
      "UA_d1 in (-90, -50) with direction preference `max` [N = 38]\n",
      "UA_d1 in (-50, -30) with direction preference `max` [N = 38]\n",
      "UA_d1 in (-60, -45) with direction preference `max` [N = 38]\n",
      "UA_d1 in (-45, -30) with direction preference `max` [N = 38]\n",
      "UA_d1 in (-30, -15) with direction preference `max` [N = 38]\n",
      "UA_d1 in (-15, 0) with direction preference `max` [N = 38]\n",
      "UA_d1 in (0, 15) with direction preference `min` [N = 38]\n",
      "UA_d1 in (15, 30) with direction preference `min` [N = 38]\n",
      "UA_d1 in (30, 45) with direction preference `min` [N = 38]\n",
      "UA_d1 in (-15, 15) with direction preference `max` [N = 38]\n",
      "UA_d1 in (-30, 30) with direction preference `max` [N = 38]\n",
      "UA_d2 in (-90, -30) with direction preference `max` [N = 272]\n",
      "UA_d2 in (-90, -50) with direction preference `max` [N = 272]\n",
      "UA_d2 in (-50, -30) with direction preference `max` [N = 272]\n",
      "UA_d2 in (-105, -90) with direction preference `max` [N = 272]\n",
      "UA_d2 in (-90, -50) with direction preference `max` [N = 272]\n",
      "UA_d2 in (-50, -30) with direction preference `max` [N = 272]\n",
      "UA_d2 in (-60, -45) with direction preference `max` [N = 272]\n",
      "UA_d2 in (-45, -30) with direction preference `max` [N = 272]\n",
      "UA_d2 in (-30, -15) with direction preference `max` [N = 272]\n",
      "UA_d2 in (-15, 0) with direction preference `max` [N = 272]\n",
      "UA_d2 in (0, 15) with direction preference `min` [N = 272]\n",
      "UA_d2 in (15, 30) with direction preference `min` [N = 272]\n",
      "UA_d2 in (30, 45) with direction preference `min` [N = 272]\n",
      "UA_d2 in (-15, 15) with direction preference `max` [N = 272]\n",
      "UA_d2 in (-30, 30) with direction preference `max` [N = 272]\n",
      "A_d0 in (-60, -15) with direction preference `max` [N = 1]\n",
      "A_d0 in (-45, -15) with direction preference `max` [N = 1]\n",
      "A_d0 in (-60, -45) with direction preference `max` [N = 1]\n",
      "A_d0 in (-45, -30) with direction preference `max` [N = 1]\n",
      "A_d0 in (-30, -15) with direction preference `max` [N = 1]\n",
      "A_d0 in (-15, 0) with direction preference `max` [N = 1]\n",
      "A_d0 in (0, 15) with direction preference `min` [N = 1]\n",
      "A_d0 in (15, 30) with direction preference `min` [N = 1]\n",
      "A_d0 in (30, 45) with direction preference `min` [N = 1]\n",
      "A_d0 in (-15, 15) with direction preference `max` [N = 1]\n",
      "A_d0 in (-30, 30) with direction preference `max` [N = 1]\n",
      "A_d1 in (-60, -15) with direction preference `max` [N = 19]\n",
      "A_d1 in (-45, -15) with direction preference `max` [N = 19]\n",
      "A_d1 in (-60, -45) with direction preference `max` [N = 19]\n",
      "A_d1 in (-45, -30) with direction preference `max` [N = 19]\n",
      "A_d1 in (-30, -15) with direction preference `max` [N = 19]\n",
      "A_d1 in (-15, 0) with direction preference `max` [N = 19]\n",
      "A_d1 in (0, 15) with direction preference `min` [N = 19]\n",
      "A_d1 in (15, 30) with direction preference `min` [N = 19]\n",
      "A_d1 in (30, 45) with direction preference `min` [N = 19]\n",
      "A_d1 in (-15, 15) with direction preference `max` [N = 19]\n",
      "A_d1 in (-30, 30) with direction preference `max` [N = 19]\n",
      "A_d2 in (-60, -15) with direction preference `max` [N = 136]\n",
      "A_d2 in (-45, -15) with direction preference `max` [N = 136]\n",
      "A_d2 in (-60, -45) with direction preference `max` [N = 136]\n",
      "A_d2 in (-45, -30) with direction preference `max` [N = 136]\n",
      "A_d2 in (-30, -15) with direction preference `max` [N = 136]\n",
      "A_d2 in (-15, 0) with direction preference `max` [N = 136]\n",
      "A_d2 in (0, 15) with direction preference `min` [N = 136]\n",
      "A_d2 in (15, 30) with direction preference `min` [N = 136]\n",
      "A_d2 in (30, 45) with direction preference `min` [N = 136]\n",
      "A_d2 in (-15, 15) with direction preference `max` [N = 136]\n",
      "A_d2 in (-30, 30) with direction preference `max` [N = 136]\n",
      "U_d0 in (-15, 15) with direction preference `max` [N = 1]\n",
      "U_d0 in (-30, 0) with direction preference `max` [N = 1]\n",
      "U_d0 in (0, 30) with direction preference `min` [N = 1]\n",
      "U_d0 in (-60, -45) with direction preference `max` [N = 1]\n",
      "U_d0 in (-45, -30) with direction preference `max` [N = 1]\n",
      "U_d0 in (-30, -15) with direction preference `max` [N = 1]\n",
      "U_d0 in (-15, 0) with direction preference `max` [N = 1]\n",
      "U_d0 in (0, 15) with direction preference `min` [N = 1]\n",
      "U_d0 in (15, 30) with direction preference `min` [N = 1]\n",
      "U_d0 in (30, 45) with direction preference `min` [N = 1]\n",
      "U_d1 in (-15, 15) with direction preference `max` [N = 19]\n",
      "U_d1 in (-30, 0) with direction preference `max` [N = 19]\n",
      "U_d1 in (0, 30) with direction preference `min` [N = 19]\n",
      "U_d1 in (-60, -45) with direction preference `max` [N = 19]\n",
      "U_d1 in (-45, -30) with direction preference `max` [N = 19]\n",
      "U_d1 in (-30, -15) with direction preference `max` [N = 19]\n",
      "U_d1 in (-15, 0) with direction preference `max` [N = 19]\n",
      "U_d1 in (0, 15) with direction preference `min` [N = 19]\n",
      "U_d1 in (15, 30) with direction preference `min` [N = 19]\n",
      "U_d1 in (30, 45) with direction preference `min` [N = 19]\n",
      "U_d2 in (-15, 15) with direction preference `max` [N = 136]\n",
      "U_d2 in (-30, 0) with direction preference `max` [N = 136]\n",
      "U_d2 in (0, 30) with direction preference `min` [N = 136]\n",
      "U_d2 in (-60, -45) with direction preference `max` [N = 136]\n",
      "U_d2 in (-45, -30) with direction preference `max` [N = 136]\n",
      "U_d2 in (-30, -15) with direction preference `max` [N = 136]\n",
      "U_d2 in (-15, 0) with direction preference `max` [N = 136]\n",
      "U_d2 in (0, 15) with direction preference `min` [N = 136]\n",
      "U_d2 in (15, 30) with direction preference `min` [N = 136]\n",
      "U_d2 in (30, 45) with direction preference `min` [N = 136]\n"
     ]
    }
   ],
   "source": [
    "for motif_type,motif_dict in gold_motifs.items():\n",
    "    for (mreg,mdir) in zip(motif_dict['regions'], motif_dict['dir_pref']):\n",
    "\n",
    "        mlist = motif_dict['motifs']\n",
    "        mdesc = f'{motif_type}_{mreg[0]}_{mreg[1]}'\n",
    "        print(f\"{motif_type} in {mreg} with direction preference `{mdir}` [N = {len(mlist)}]\")\n",
    "\n",
    "        gold_data[f'idxs_{mdesc}']     = gold_data.apply(lambda row: cleavage.find_motifs_locations(row, 'ctr_cleavage', mreg, mlist, 'equal', False, mdir, over_right_edge = True), axis = 1)\n",
    "        gold_data[f'idx_{mdesc}']      = gold_data[f'idxs_{mdesc}'].apply(lambda x : cleavage.select_motifs(x, index = mdir, relative = False))\n",
    "        gold_data[f'count_{mdesc}']    = gold_data.apply(lambda row: cleavage.count_motifs(row['sequence'][max([0,int(row['ctr_cleavage']+mreg[0])]):min([len(row['sequence']),int(row['ctr_cleavage']+mreg[1]+5)])], mlist, method = 'equal', overlapping = False, preference = motif_dict['dir_pref'], count_only = True), axis = 1)\n",
    "        gold_data[f'multiple_{mdesc}'] = (gold_data[f'count_{mdesc}'] > 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantify the nucleotide density surrounding the max cleavage site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_data['nd_A'] = gold_data.apply(lambda row : row['sequence'][int(row['ctr_cleavage']-15):int(row['ctr_cleavage']+15)].count(\"A\") / 30 * 100, axis = 1)\n",
    "gold_data['nd_C'] = gold_data.apply(lambda row : row['sequence'][int(row['ctr_cleavage']-15):int(row['ctr_cleavage']+15)].count(\"C\") / 30 * 100, axis = 1)\n",
    "gold_data['nd_G'] = gold_data.apply(lambda row : row['sequence'][int(row['ctr_cleavage']-15):int(row['ctr_cleavage']+15)].count(\"G\") / 30 * 100, axis = 1)\n",
    "gold_data['nd_T'] = gold_data.apply(lambda row : row['sequence'][int(row['ctr_cleavage']-15):int(row['ctr_cleavage']+15)].count(\"T\") / 30 * 100, axis = 1)\n",
    "\n",
    "gold_data['nd_T_up'] = gold_data.apply(lambda row : row['sequence'][int(row['ctr_cleavage']-15):int(row['ctr_cleavage'])].count(\"T\") / 15 * 100, axis = 1)\n",
    "gold_data['nd_T_dn'] = gold_data.apply(lambda row : row['sequence'][int(row['ctr_cleavage']):int(row['ctr_cleavage']+15)].count(\"T\") / 15 * 100, axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify strongly positive golden sites with high classification confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden sites:\t (11673, 442)\n",
      "TP golden sites: (11400, 442)\n"
     ]
    }
   ],
   "source": [
    "gold_tpdata = gold_data.loc[gold_data['classification'] >= 0.9].copy()\n",
    "print(\"Golden sites:\\t\", gold_data.shape)\n",
    "print(\"TP golden sites:\", gold_tpdata.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate specific motif distances for UA/UA-rich repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_tpdata['first_UA_d2_-90_-30']      = gold_tpdata['idxs_UA_d2_-90_-30'].apply(lambda x : cleavage.select_motifs(x, 'first'))\n",
    "gold_tpdata['last_UA_d2_-90_-30']       = gold_tpdata['idxs_UA_d2_-90_-30'].apply(lambda x : cleavage.select_motifs(x, 'last'))\n",
    "gold_tpdata['secondlast_UA_d2_-90_-30'] = gold_tpdata['idxs_UA_d2_-90_-30'].apply(lambda x : cleavage.select_motifs(x, 'second_last'))\n",
    "\n",
    "gold_tpdata['dist_first_UA_d2_-90_-30_CS']      = np.abs(gold_tpdata['ctr_cleavage'] - gold_tpdata['first_UA_d2_-90_-30']) - 6\n",
    "gold_tpdata['dist_last_UA_d2_-90_-30_CS']       = np.abs(gold_tpdata['ctr_cleavage'] - gold_tpdata['last_UA_d2_-90_-30']) - 6\n",
    "gold_tpdata['dist_secondlast_UA_d2_-90_-30_CS'] = np.abs(gold_tpdata['ctr_cleavage'] - gold_tpdata['secondlast_UA_d2_-90_-30']) - 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_tpdata['first_UA_d1_-90_-30']      = gold_tpdata['idxs_UA_d1_-90_-30'].apply(lambda x : cleavage.select_motifs(x, 'first'))\n",
    "gold_tpdata['last_UA_d1_-90_-30']       = gold_tpdata['idxs_UA_d1_-90_-30'].apply(lambda x : cleavage.select_motifs(x, 'last'))\n",
    "gold_tpdata['secondlast_UA_d1_-90_-30'] = gold_tpdata['idxs_UA_d1_-90_-30'].apply(lambda x : cleavage.select_motifs(x, 'second_last'))\n",
    "\n",
    "gold_tpdata['dist_first_UA_d1_-90_-30_CS']      = np.abs(gold_tpdata['ctr_cleavage'] - gold_tpdata['first_UA_d1_-90_-30']) - 6\n",
    "gold_tpdata['dist_last_UA_d1_-90_-30_CS']       = np.abs(gold_tpdata['ctr_cleavage'] - gold_tpdata['last_UA_d1_-90_-30']) - 6\n",
    "gold_tpdata['dist_secondlast_UA_d1_-90_-30_CS'] = np.abs(gold_tpdata['ctr_cleavage'] - gold_tpdata['secondlast_UA_d1_-90_-30']) - 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirm that motif distances were calculated correctly - expect an empty data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ctr_cleavage</th>\n",
       "      <th>count_UA_d1_-90_-30</th>\n",
       "      <th>idxs_UA_d1_-90_-30</th>\n",
       "      <th>first_UA_d1_-90_-30</th>\n",
       "      <th>last_UA_d1_-90_-30</th>\n",
       "      <th>secondlast_UA_d1_-90_-30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ctr_cleavage, count_UA_d1_-90_-30, idxs_UA_d1_-90_-30, first_UA_d1_-90_-30, last_UA_d1_-90_-30, secondlast_UA_d1_-90_-30]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cond1 = ~gold_tpdata['dist_first_UA_d1_-90_-30_CS'].isna() & ~gold_tpdata['dist_first_UA_d1_-90_-30_CS'].between(25,90)\n",
    "cond2 = ~gold_tpdata['dist_secondlast_UA_d1_-90_-30_CS'].isna() & ~gold_tpdata['dist_secondlast_UA_d1_-90_-30_CS'].between(25,90)\n",
    "cond3 = ~gold_tpdata['dist_last_UA_d1_-90_-30_CS'].isna() & ~gold_tpdata['dist_last_UA_d1_-90_-30_CS'].between(25,90)\n",
    "\n",
    "(gold_tpdata\n",
    " .loc[cond1 | cond2 | cond3]\n",
    " [['ctr_cleavage','count_UA_d1_-90_-30','idxs_UA_d1_-90_-30','first_UA_d1_-90_-30','last_UA_d1_-90_-30','secondlast_UA_d1_-90_-30']]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split sites into low, middle, and high entropy groups based on observed entropy only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    2280\n",
      "2    6840\n",
      "3    2280\n",
      "Name: observed_entropy_group, dtype: int64\n",
      "1    2367\n",
      "2    6966\n",
      "3    2340\n",
      "Name: observed_entropy_group, dtype: int64\n",
      "\n",
      "Observed entropy group cutoffs: [0.63497327 2.19106129 2.83239175 3.69158609]\n"
     ]
    }
   ],
   "source": [
    "gold_tpdata['observed_entropy_group'], observed_entropy_group_bins = pd.qcut(gold_tpdata['observed_entropy'], q = [0,0.2,0.8,1], labels = range(1,4), retbins=True)\n",
    "print(gold_tpdata['observed_entropy_group'].value_counts(sort = False))\n",
    "\n",
    "gold_data['observed_entropy_group'] = pd.cut(gold_data['observed_entropy'], bins=[0, observed_entropy_group_bins[1], observed_entropy_group_bins[2], 4], labels = range(1,4))\n",
    "print(gold_data['observed_entropy_group'].value_counts(sort = False))\n",
    "\n",
    "print(\"\\nObserved entropy group cutoffs:\", observed_entropy_group_bins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split sites into low, middle, and high entropy groups based on predicted entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    2280\n",
      "2    6840\n",
      "3    2280\n",
      "Name: predicted_entropy_group, dtype: int64\n",
      "1    2347\n",
      "2    6989\n",
      "3    2337\n",
      "Name: predicted_entropy_group, dtype: int64\n",
      "\n",
      "Predicted entropy group cutoffs: [0.38364624 1.64297899 2.27988049 2.94592164]\n"
     ]
    }
   ],
   "source": [
    "gold_tpdata['predicted_entropy_group'], predicted_entropy_group_bins = pd.qcut(gold_tpdata['predicted_entropy'], q = [0,0.2,0.8,1], labels = range(1,4), retbins=True)\n",
    "print(gold_tpdata['predicted_entropy_group'].value_counts(sort = False))\n",
    "\n",
    "gold_data['predicted_entropy_group'] = pd.cut(gold_data['predicted_entropy'], bins=[0, predicted_entropy_group_bins[1], predicted_entropy_group_bins[2], 4], labels = range(1,4))\n",
    "print(gold_data['predicted_entropy_group'].value_counts(sort = False))\n",
    "\n",
    "print(\"\\nPredicted entropy group cutoffs:\", predicted_entropy_group_bins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify sites that are in consensus entropy groups for both observed and predicted entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True     7536\n",
      "False    3864\n",
      "Name: consensus_entropy, dtype: int64\n",
      "True     7702\n",
      "False    3971\n",
      "Name: consensus_entropy, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "gold_tpdata['consensus_entropy'] = (gold_tpdata['observed_entropy_group'] == gold_tpdata['predicted_entropy_group'])\n",
    "print(gold_tpdata['consensus_entropy'].value_counts())\n",
    "\n",
    "gold_data['consensus_entropy'] = (gold_data['observed_entropy_group'] == gold_data['predicted_entropy_group'])\n",
    "print(gold_data['consensus_entropy'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Record results and save entropy group labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTDIR, 'gold_data.ctr.pickle'), mode = 'wb') as handle:\n",
    "    pickle.dump(gold_data, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(OUTDIR, 'gold_data.cleavage_heterogeneity.ctr.pickle'), mode = 'wb') as handle:\n",
    "    pickle.dump(gold_tpdata, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4560\n"
     ]
    }
   ],
   "source": [
    "gold_hilo = gold_tpdata.loc[gold_tpdata['observed_entropy_group'].isin([1,3]), ['label','observed_entropy_group']].copy()\n",
    "gold_dict = dict(zip(gold_hilo['label'], gold_hilo['observed_entropy_group']))\n",
    "\n",
    "with open(os.path.join(OUTDIR, 'gold_data.entropy_groups.ctr.pickle'), mode = 'wb') as handle:\n",
    "    pickle.dump(gold_dict, handle)\n",
    "    \n",
    "print(len(gold_dict)) ## expecting 4560\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save datasets for cleavage mechanism analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High entropy site selection: 2280 7536 2717 305\n",
      "Low entropy site selection: 2280 7536 1355\n",
      "Sites selected for analysis: (1660, 457)\n"
     ]
    }
   ],
   "source": [
    "## Subset sites with a single upstream efficiency element\n",
    "\n",
    "# High entropy sites\n",
    "\n",
    "cond0 = (gold_tpdata['observed_entropy_group'] == 3)\n",
    "cond1 = (gold_tpdata['consensus_entropy'] == True)\n",
    "cond2 = (gold_tpdata['count_UA_d2_-90_-30'] == 5)\n",
    "\n",
    "condHIGH = (cond0 & cond1 & cond2)\n",
    "print(\"High entropy site selection:\", cond0.sum(), cond1.sum(), cond2.sum(), condHIGH.sum())\n",
    "\n",
    "# Low entropy sites\n",
    "\n",
    "cond0 = (gold_tpdata['observed_entropy_group'] == 1)\n",
    "cond1 = (gold_tpdata['consensus_entropy'] == True)\n",
    "\n",
    "condLOW = (cond0 & cond1)\n",
    "print(\"Low entropy site selection:\", cond0.sum(), cond1.sum(), condLOW.sum())\n",
    "\n",
    "# Combine sites selected for analysis\n",
    "\n",
    "out_data = gold_tpdata.loc[condHIGH | condLOW].copy().rename(columns = {\n",
    "    'idxs_UA_d2_-90_-30'  : 'idxs_EE', \n",
    "    'idx_UA_d2_-90_-30'   : 'idx_EE', \n",
    "    'count_UA_d2_-90_-30' : 'count_EE',\n",
    "    'idxs_A_d2_-45_-15'   : 'idxs_PE',\n",
    "    'idx_A_d2_-45_-15'    : 'idx_PE',\n",
    "    'count_A_d2_-45_-15'  : 'count_PE',\n",
    "}).copy()\n",
    "\n",
    "print(\"Sites selected for analysis:\", out_data.shape)\n",
    "\n",
    "with open(os.path.join(OUTDIR, 'gold_tpdata.for_uaua_analysis.d2.ctr.pickle'), mode = 'wb') as handle:\n",
    "    pickle.dump(out_data, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High entropy site selection: 2280 7536 2029 7392 7214 182\n",
      "Low entropy site selection : 2280 7536 7287 6760 4008 4186 222\n",
      "Sites selected for analysis: (404, 457)\n"
     ]
    }
   ],
   "source": [
    "## Subset sites for cleavage site composition analysis\n",
    "\n",
    "# High entropy sites\n",
    "\n",
    "cond0 = (gold_tpdata['observed_entropy_group'] == 3)\n",
    "cond1 = (gold_tpdata['consensus_entropy'] == True)\n",
    "cond2 = (gold_tpdata['count_U_d2_-15_15'] == 0)\n",
    "cond3 = (gold_tpdata['count_UA_d2_-15_0'] > 0)\n",
    "cond4 = (gold_tpdata['count_UA_d2_0_15'] > 0)\n",
    "\n",
    "condHIGH = (cond0 & cond1 & cond2 & cond3 & cond4)\n",
    "print(\"High entropy site selection:\", cond0.sum(), cond1.sum(), cond2.sum(), cond3.sum(), cond4.sum(), condHIGH.sum())\n",
    "\n",
    "# Low entropy sites\n",
    "\n",
    "cond0 = (gold_tpdata['observed_entropy_group'] == 1)\n",
    "cond1 = (gold_tpdata['consensus_entropy'] == True)\n",
    "cond2 = (gold_tpdata['count_U_d2_-15_0'] > 0)\n",
    "cond3 = (gold_tpdata['count_U_d2_0_15'] > 0)\n",
    "cond4 = (gold_tpdata['count_UA_d2_-15_0'] == 0)\n",
    "cond5 = (gold_tpdata['count_UA_d2_0_15'] == 0)\n",
    "\n",
    "condLOW = (cond0 & cond1 & cond2 & cond3 & cond4 & cond5)\n",
    "print(\"Low entropy site selection :\", cond0.sum(), cond1.sum(), cond2.sum(), cond3.sum(), cond4.sum(), cond5.sum(), condLOW.sum())\n",
    "\n",
    "# Combine sites selected for analysis\n",
    "\n",
    "out_data = gold_tpdata.loc[condLOW | condHIGH].copy()\n",
    "print(\"Sites selected for analysis:\", out_data.shape)\n",
    "\n",
    "with open(os.path.join(OUTDIR, 'gold_tpdata.for_uf_df_analysis.d2.ctr.pickle'), mode = 'wb') as handle:\n",
    "    pickle.dump(out_data, handle)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-train)",
   "language": "python",
   "name": "tf-train"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
